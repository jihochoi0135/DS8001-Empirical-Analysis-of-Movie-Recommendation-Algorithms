# -*- coding: utf-8 -*-
"""item-based.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sYrR3pWcD6UUcLs0QyRn-aYdTClEkcRG
"""

import os
import pandas as pd
import numpy as np
import time
import random

# Step 0: Same pre-procedure for every algorithms

def rmse(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    return np.sqrt(np.mean((y_true - y_pred) ** 2))


def split_train_test(df, test_ratio=0.2, seed=42):
    df_shuffled = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)
    n_test = int(len(df) * test_ratio)

    test = df_shuffled.iloc[:n_test].copy()
    train = df_shuffled.iloc[n_test:].copy()

    mask = test["movieId"].isin(train["movieId"]) & test["userId"].isin(train["userId"])
    test = test[mask].reset_index(drop=True)

    return train, test


def evaluate_model(df, predict_fn, test_ratio=0.2, seed=42):
    start_time = time.time()
    train, test = split_train_test(df, test_ratio=test_ratio, seed=seed)
    y_true = test["rating"].values
    y_pred = predict_fn(train, test)
    rmse_score = rmse(y_true, y_pred)
    elapsed = time.time() - start_time
    return rmse_score, elapsed, train, test, y_pred


# item-based
def predict_item_based(train, test, k=30):

    # common procedure as user-based:
    # mapped userId and movieId with their index
    user_ids = train["userId"].unique()
    item_ids = train["movieId"].unique()

    user2idx = {u: idx for idx, u in enumerate(user_ids)}
    item2idx = {m: idx for idx, m in enumerate(item_ids)}

    n_users = len(user_ids)
    n_items = len(item_ids)

    # user-item rates rows (train set)
    R = np.full((n_users, n_items), np.nan, dtype=float)

    for row in train.itertuples(index=False):
        u = user2idx[row.userId]
        i = item2idx[row.movieId]
        R[u, i] = row.rating

    # user-mean and get difference
    global_mean = np.nanmean(R)
    item_mean = np.nanmean(R, axis=0)
    item_mean = np.where(np.isnan(item_mean), global_mean, item_mean)

    R_diff = R - item_mean[None, :]
    R_diff = np.nan_to_num(R_diff, nan=0.0)

    #common procedure as user-based: end#


    # cosine similarity for item-item
    item_norms = np.linalg.norm(R_diff, axis=0)
    item_norms[item_norms == 0] = 1e-8

    # R_diff.T: (n_items x n_users)
    sim_items = R_diff.T @ R_diff
    sim_items = sim_items / (item_norms[:, None] * item_norms[None, :])
    np.fill_diagonal(sim_items, 0.0)

    # Prediction function for individual (userId, movieId)
    def predict_single(user_id, movie_id):
        if user_id not in user2idx or movie_id not in item2idx:
            return float(global_mean)

        u_idx = user2idx[user_id]
        i_idx = item2idx[movie_id]

        # the items that the user 'u' rated.
        user_ratings = R[u_idx, :]
        rated_mask = ~np.isnan(user_ratings)
        if not np.any(rated_mask):
            return float(item_mean[i_idx]) if not np.isnan(item_mean[i_idx]) else float(global_mean)

        sims = sim_items[i_idx, :].copy()
        sims[~rated_mask] = 0.0

        if np.all(sims == 0):
            return float(item_mean[i_idx]) if not np.isnan(item_mean[i_idx]) else float(global_mean)

        # Top-K
        neighbor_idx = np.argsort(-np.abs(sims))[:k]
        neighbor_sims = sims[neighbor_idx]

        neighbor_ratings = user_ratings[neighbor_idx]
        neighbor_means   = item_mean[neighbor_idx]

        dev = neighbor_ratings - neighbor_means
        mask = ~np.isnan(dev)

        if not np.any(mask):
            return float(item_mean[i_idx]) if not np.isnan(item_mean[i_idx]) else float(global_mean)

        neighbor_sims = neighbor_sims[mask]
        dev = dev[mask]

        denom = np.sum(np.abs(neighbor_sims))
        if denom == 0:
            return float(item_mean[i_idx]) if not np.isnan(item_mean[i_idx]) else float(global_mean)

        pred = item_mean[i_idx] + np.sum(neighbor_sims * dev) / denom
        return float(pred)

    # predicts on whole test dataframe
    preds = []
    for row in test.itertuples(index=False):
        preds.append(predict_single(row.userId, row.movieId))

    return np.array(preds, dtype=float)



#==small
os.chdir(r"C:/Users/Jason/OneDrive/TMU/Fall 25/DS8001/Project/ml-latest-small")
df = pd.read_csv("Master_small.csv")
seed = random.randint(1, 10_000_000)
rmse_val, time_taken,_,_,_ = evaluate_model(df,predict_item_based,0.2,seed)
print(f"Naive Small: RMSE={rmse_val:.4f}, time={time_taken:.4f}s")


print("loading big set")
os.chdir(r"C:/Users/Jason/OneDrive/TMU/Fall 25/DS8001/Project/ml-latest")
df = pd.read_csv("Master_large.csv")
print("done loading big set")
sizes = [100000, 390000, 680000, 970000, 1260000,
    1550000, 1840000, 2130000, 2420000, 2710000,
    3000000]
results = []
for n in sizes:
    print(f"\n=== Running on first {n:,} rows ===")
    df_sub = df.iloc[:n].copy()
    seed = random.randint(1, 10_000_000)
    rmse_val, time_taken, _, _,_ = evaluate_model(
        df_sub,
        predict_item_based,
        0.2,
        seed
    )
    print(f"item ({n:,} rows): RMSE={rmse_val:.4f}, time={time_taken:.4f}s")
    results.append({
        "size": n,
        "rmse": rmse_val,
        "time_seconds": time_taken
    })

results_df = pd.DataFrame(results)
os.chdir(r"C:/Users/Jason/OneDrive/TMU/Fall 25/DS8001/Project")
results_df.to_csv("item_results.csv", index=False)
print("\nSaved results to item_results.csv")



seed = random.randint(1, 10_000_000)
rmse_val, time_taken,_,_,_ = evaluate_model(df,predict_item_based,0.2,seed)
print(f"item Large: RMSE={rmse_val:.4f}, time={time_taken:.4f}s")


# Please replace csv name with the path
'''
# run with Master_small.csv
if __name__ == "__main__":
    df = pd.read_csv("Master_small.csv")[["userId", "movieId", "rating"]].copy()

    ib_rmse, ib_time, train_df, test_df, y_pred = evaluate_model(
        df,
        predict_fn=predict_item_based,
        test_ratio=0.2,
        seed=42
    )

    print(f"Item-based Small: Test RMSE: {ib_rmse:.4f}")
    print(f"Item-based Small: Total time (train+test): {ib_time:.3f} s")
'''
